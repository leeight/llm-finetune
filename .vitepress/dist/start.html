<!DOCTYPE html>
<html lang="zh" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>大模型微调入门 | 大模型微调与部署指南</title>
    <meta name="description" content="A VitePress site">
    <meta name="generator" content="VitePress v1.6.3">
    <link rel="preload stylesheet" href="/llm-finetune/assets/style.CcTc65Li.css" as="style">
    <link rel="preload stylesheet" href="/llm-finetune/vp-icons.css" as="style">
    
    <script type="module" src="/llm-finetune/assets/app.C6acgjee.js"></script>
    <link rel="preload" href="/llm-finetune/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/llm-finetune/assets/chunks/theme.Da26EFXg.js">
    <link rel="modulepreload" href="/llm-finetune/assets/chunks/framework.C1nslR49.js">
    <link rel="modulepreload" href="/llm-finetune/assets/start.md.BJOPEC1V.lean.js">
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"light",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-d8b57b2d><!--[--><!--]--><!--[--><span tabindex="-1" data-v-fcbfc0e0></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-fcbfc0e0>Skip to content</a><!--]--><!----><header class="VPNav" data-v-d8b57b2d data-v-7ad780c2><div class="VPNavBar" data-v-7ad780c2 data-v-9fd4d1dd><div class="wrapper" data-v-9fd4d1dd><div class="container" data-v-9fd4d1dd><div class="title" data-v-9fd4d1dd><div class="VPNavBarTitle has-sidebar" data-v-9fd4d1dd data-v-9f43907a><a class="title" href="/llm-finetune/" data-v-9f43907a><!--[--><!--]--><!--[--><img class="VPImage logo" src="/llm-finetune/logo.png" alt data-v-ab19afbb><!--]--><span data-v-9f43907a>大模型微调与部署指南</span><!--[--><!--]--></a></div></div><div class="content" data-v-9fd4d1dd><div class="content-body" data-v-9fd4d1dd><!--[--><!--]--><div class="VPNavBarSearch search" data-v-9fd4d1dd><!--[--><!----><div id="local-search"><button type="button" class="DocSearch DocSearch-Button" aria-label="搜索"><span class="DocSearch-Button-Container"><span class="vp-icon DocSearch-Search-Icon"></span><span class="DocSearch-Button-Placeholder">搜索</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-9fd4d1dd data-v-afb2845e><span id="main-nav-aria-label" class="visually-hidden" data-v-afb2845e> Main Navigation </span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/llm-finetune/" tabindex="0" data-v-afb2845e data-v-815115f5><!--[--><span data-v-815115f5>首页</span><!--]--></a><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-9fd4d1dd data-v-3f90c1a5><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-3f90c1a5 data-v-be9742d9 data-v-b4ccac88><span class="check" data-v-b4ccac88><span class="icon" data-v-b4ccac88><!--[--><span class="vpi-sun sun" data-v-be9742d9></span><span class="vpi-moon moon" data-v-be9742d9></span><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-9fd4d1dd data-v-ef6192dc data-v-e71e869c><!--[--><a class="VPSocialLink no-icon" href="https://github.com/nwind/llm-finetune" aria-label="github" target="_blank" rel="noopener" data-v-e71e869c data-v-60a9a2d3><span class="vpi-social-github"></span></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-9fd4d1dd data-v-f953d92f data-v-bfe7971f><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-bfe7971f><span class="vpi-more-horizontal icon" data-v-bfe7971f></span></button><div class="menu" data-v-bfe7971f><div class="VPMenu" data-v-bfe7971f data-v-20ed86d6><!----><!--[--><!--[--><!----><div class="group" data-v-f953d92f><div class="item appearance" data-v-f953d92f><p class="label" data-v-f953d92f>Appearance</p><div class="appearance-action" data-v-f953d92f><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-f953d92f data-v-be9742d9 data-v-b4ccac88><span class="check" data-v-b4ccac88><span class="icon" data-v-b4ccac88><!--[--><span class="vpi-sun sun" data-v-be9742d9></span><span class="vpi-moon moon" data-v-be9742d9></span><!--]--></span></span></button></div></div></div><div class="group" data-v-f953d92f><div class="item social-links" data-v-f953d92f><div class="VPSocialLinks social-links-list" data-v-f953d92f data-v-e71e869c><!--[--><a class="VPSocialLink no-icon" href="https://github.com/nwind/llm-finetune" aria-label="github" target="_blank" rel="noopener" data-v-e71e869c data-v-60a9a2d3><span class="vpi-social-github"></span></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-9fd4d1dd data-v-6bee1efd><span class="container" data-v-6bee1efd><span class="top" data-v-6bee1efd></span><span class="middle" data-v-6bee1efd></span><span class="bottom" data-v-6bee1efd></span></span></button></div></div></div></div><div class="divider" data-v-9fd4d1dd><div class="divider-line" data-v-9fd4d1dd></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-d8b57b2d data-v-2488c25a><div class="container" data-v-2488c25a><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-2488c25a><span class="vpi-align-left menu-icon" data-v-2488c25a></span><span class="menu-text" data-v-2488c25a>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-2488c25a data-v-6b867909><button data-v-6b867909>Return to top</button><!----></div></div></div><aside class="VPSidebar" data-v-d8b57b2d data-v-42c4c606><div class="curtain" data-v-42c4c606></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-42c4c606><span class="visually-hidden" id="sidebar-aria-label" data-v-42c4c606> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-51288d80><section class="VPSidebarItem level-0 has-active" data-v-51288d80 data-v-0009425e><!----><div class="items" data-v-0009425e><!--[--><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/llm-finetune/intro.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>前言</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/llm-finetune/start.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>大模型微调入门</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/llm-finetune/basic.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>大模型基础</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/llm-finetune/sft.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>微调训练</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/llm-finetune/rl.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>对齐训练</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/llm-finetune/data.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>训练数据构造及管理</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/llm-finetune/eval.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>评估</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/llm-finetune/practice.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>微调实践</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/llm-finetune/deploy.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>模型部署</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/llm-finetune/appendix.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>附录</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/llm-finetune/extend.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>拓展阅读</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-d8b57b2d data-v-9a6c75ad><div class="VPDoc has-sidebar has-aside" data-v-9a6c75ad data-v-e6f2a212><!--[--><!--]--><div class="container" data-v-e6f2a212><div class="aside" data-v-e6f2a212><div class="aside-curtain" data-v-e6f2a212></div><div class="aside-container" data-v-e6f2a212><div class="aside-content" data-v-e6f2a212><div class="VPDocAside" data-v-e6f2a212 data-v-cb998dce><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-cb998dce data-v-f610f197><div class="content" data-v-f610f197><div class="outline-marker" data-v-f610f197></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-f610f197>On this page</div><ul class="VPDocOutlineItem root" data-v-f610f197 data-v-53c99d69><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-cb998dce></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-e6f2a212><div class="content-container" data-v-e6f2a212><!--[--><!--]--><main class="main" data-v-e6f2a212><div style="position:relative;" class="vp-doc _llm-finetune_start" data-v-e6f2a212><div><h1 id="大模型微调入门" tabindex="-1">大模型微调入门 <a class="header-anchor" href="#大模型微调入门" aria-label="Permalink to &quot;大模型微调入门&quot;">​</a></h1><p>本章将通过实践入门大模型微调。大模型微调是一项实践性很强的科学，笔者建议首先进行几次实际操作，获得初步体验后再学习理论知识，这样效果会更好。</p><h2 id="有哪些大模型微调方法" tabindex="-1">有哪些大模型微调方法 <a class="header-anchor" href="#有哪些大模型微调方法" aria-label="Permalink to &quot;有哪些大模型微调方法&quot;">​</a></h2><p>大模型微调有多种实现方法，从简单到复杂，主要包括以下几种：</p><ol><li><strong>使用公有云平台微调</strong>，如 OpenAI 的 Fine-tuning、百度的千帆、阿里的百炼等。这是最简单的方式，只需准备数据并上传到云平台后运行，支持少量超参数配置。</li><li><strong>使用 torchtune 或 LLaMA-Factory 等工具进行微调</strong>，这些工具只需准备数据和硬件即可微调大模型，无需开发成本，仅需了解超参数配置。</li><li><strong>使用 Transformers 库</strong>，相比于 LLaMA-Factory 更为底层，需要编写训练相关的代码。由于屏蔽了过多细节，不适合用来深入理解微调原理。</li><li><strong>使用 PyTorch 实现大模型</strong>，需要从零实现大模型架构，开发成本较高，但非常适合用来学习大模型原理。通常，了解原理到这一层就足够了，因此本书在介绍原理时会采用这种方式。</li><li><strong>使用 C++/CUDA 实现大模型</strong>，这是最底层的实现，开发成本最高，但性能也最好，通常用于对性能要求高的推理场景。需要掌握硬件知识，本书不涉及此部分，推荐通过 llm.c 和 ZhiLight 项目进行学习。</li></ol><p>接下来，本章将介绍如何使用公有云平台及 LLaMA-Factory 进行微调。</p><h2 id="使用公有云平台进行微调" tabindex="-1">使用公有云平台进行微调 <a class="header-anchor" href="#使用公有云平台进行微调" aria-label="Permalink to &quot;使用公有云平台进行微调&quot;">​</a></h2><p>微调最简单的方法是使用公有云平台。目前国内许多模型厂商都开放了大模型微调能力，其中比较著名的是百度千帆和阿里百炼。与其他大模型创业公司相比，这两个平台支持的模型数量较多，功能也较为全面。</p><p>这两个平台的功能对比如下：</p><ul><li>支持微调的模型 <ul><li>百度千帆 <ul><li>文心系列，包括 Turbo、Speed、Lite 等</li><li>Llama 2、Llama 3 的 8B、13B 等</li><li>Mixtral-8x7B</li><li>SQLCoder-7B</li><li>ChatGLM-2 和 ChatGLM3 的 6B</li><li>Baichuan2-7B</li><li>CodeLlama-7B</li><li>BLOOMZ-7B</li></ul></li><li>阿里百炼 <ul><li>通义千问开源模型的 7B、14B、72B</li><li>通义千问闭源模型 Turbo、Plus</li><li>Baichuan2-7B</li><li>Llama 2 的 7B 和 13B</li><li>ChatGLM 2 的 6B</li></ul></li></ul></li><li>训练方法 <ul><li>百度千帆 <ul><li>增量预训练</li><li>指令微调</li><li>偏好对齐：KTO、SimPO、DPO、RLHF</li></ul></li><li>阿里百炼 <ul><li>指令微调</li><li>偏好对齐：DPO</li></ul></li></ul></li><li>数据处理 <ul><li>百度千帆 <ul><li>数据增强</li><li>人工标注服务（众测）</li><li>数据回流</li></ul></li><li>阿里百炼 <ul><li>数据增强</li><li>数据清洗</li></ul></li></ul></li></ul><p>从对比可以看出，在微调方面百度千帆功能最为丰富，支持的模型和微调方法也最多。然而，其中许多模型目前已经被淘汰（例如 BLOOMZ 和 SQLCoder-7B）。因此，这两个平台最大的差异在于专有模型的支持，只有千帆支持文心模型，只有百炼支持通义千问模型。与文心模型相比，通义千问有开源版本，我们不仅能在百炼上进行训练，还能在本机上训练。为了后续章节的一致性，这里将使用百炼平台作为示例进行介绍，百度千帆的操作方法也类似。</p><p>首先，创建数据集，访问「数据中心」-「模型数据」。</p><p><img src="/llm-finetune/assets/bailian_data.Btzhh6OX.png" alt="上传数据"></p><p>点击底部的「JSON 数据模板」可以下载 JSONL 示例，这是名为 ChatML 的格式，最早在 OpenAI 中使用。每一行内容如下所示：</p><div class="language-json vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">json</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">{</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  &quot;messages&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: [</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    {</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">      &quot;role&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;system&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">      &quot;content&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;You are a helpful assistant&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    },</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    {</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">      &quot;role&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;user&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">      &quot;content&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;谁在文艺复兴时期绘制人体?&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    },</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    {</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">      &quot;role&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;assistant&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">      &quot;content&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;文艺复兴时期是一个关于艺术、文化和学术的复兴运动，在这个时期，许多艺术家都绘制了人体。&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    },</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    {</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">      &quot;role&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;user&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">      &quot;content&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;那雕塑方面如何呢？&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    },</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    {</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">      &quot;role&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;assistant&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">      &quot;content&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;文艺复兴时期的雕塑也非常有名，几位世界级的雕塑大师都出自于这个时期。&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    }</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  ]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">}</span></span></code></pre></div><p>上面的内容是两轮对话数据。为了简化，这里我们不对示例数据进行修改，直接将下载后的文件上传，这时就具备了首个训练数据。上传后的数据可以进行二次编辑，但要想用这个数据进行微调，需要先点击「发布」，如下图所示。</p><p><img src="/llm-finetune/assets/bailian_data_release.Dk_v4dPv.png" alt="发布数据"></p><p>接下来，进入「模型工具」-「模型调优」界面进行微调，预制模型使用「通义千问 2.5-14B」，训练数据使用刚才发布的 V1 版本。</p><p><img src="/llm-finetune/assets/bailian_train.jpeOMNI1.png" alt="训练"></p><p>然后点击「开始训练」即可。这个简单的示例只有 55 条（其中大部分是重复的），训练耗时半小时，花费不到 1 元。</p><p>训练之后的模型可以在「模型工具」-「模型部署」页面进行部署，如下图所示。</p><p><img src="/llm-finetune/assets/bailian_deploy.D3Ted8db.png" alt="部署"></p><p>可以看到，使用平台进行微调非常简单，甚至完全不懂技术的人也能操作。只需使用 Excel 编辑训练数据，然后上传到平台后就能进行训练。</p><h2 id="本地微调工具选型" tabindex="-1">本地微调工具选型 <a class="header-anchor" href="#本地微调工具选型" aria-label="Permalink to &quot;本地微调工具选型&quot;">​</a></h2><p>虽然使用平台微调工具很简单，但也有不少缺点：</p><ul><li><strong>价格较贵</strong>：相比于自己部署，平台的价格更高，毕竟平台需要通过差价来弥补研发成本。例如，在阿里百炼上部署每月的费用是 20000 元，但如果购买单卡 A10 GPU 的 ECS 实例只需不到 5000 元，这样的配置足以运行 7B 模型。</li><li><strong>不支持私有部署</strong>：训练好的模型无法下载，大部分平台不支持私有部署，即便支持，售价也很高。</li><li><strong>模型有限</strong>：只能使用平台提供的模型，数量较少。</li><li><strong>可控性低</strong>：可调的超参数较少，比如 OpenAI 只允许设置 3 个超参数。</li></ul><p>我们完全可以用开源工具实现相同功能，甚至可能做得更好。那么有哪些本地微调工具值得关注呢？以下是笔者认为值得关注的 4 类工具：</p><ul><li><p><strong>TRL</strong>：由 Hugging Face 维护的微调工具，支持指令微调、SFT、RLHF、DPO 等方法，核心开发者预计不超过 10 人。</p><ul><li>优点：功能全面，几乎支持所有模型，生态完善，许多新的微调技术会优先在 TRL 项目中实现。</li><li>缺点：较为底层，许多功能需要编写代码实现，并且依赖 Transformers 项目的模型实现，导致难以对单个模型做特殊优化，例如后面提到的打包多个指令的注意力优化。</li></ul></li><li><p>基于 Transformers 及 TRL 生态扩展的 <strong>LLaMA-Factory</strong>、<strong>Axolotl</strong>、<strong>Firefly</strong>、<strong>unsloth</strong> 和 <strong>ms-swift</strong> 等工具，它们的特点是在 SFT 微调上进行了自我实现，而 DPO、PPO 则是基于 TRL 项目改造。原因是早期 TRL 库的定位仅限于强化学习，未实现 SFT 微调。</p><ul><li>优点：继承了 TRL 的优点，相较于 TRL 更加上层，提供基于配置文件的微调，使用起来更简单，无需编写代码，并且集成了许多优化以提升训练性能。</li><li>缺点：随着 TRL 功能的完善，这些库的优势会逐渐减小。此外，这些库的开发人员相对较少，大多只有 1-2 名主力开发者，其它人的代码贡献量较小，且以学生为主，毕业后是否继续维护存在不确定性。为了实现部分优化，这些库需要对 Transformers 中的模型进行修改（主要是注意力机制），因此对 Transformers 版本进行了限制，难以使用最新版本。</li></ul></li><li><p><strong>torchtune</strong>：由 PyTorch 官方维护的微调工具，支持大部分微调方法，核心开发者预计不到 10 人。</p><ul><li>优点：代码精简，外部依赖少，技术实力强，已实现大量显存优化技术，灵活性高，除了可以通过配置方式使用外，还可以通过编写代码来完全控制训练过程。</li><li>缺点：使用人数不多，对新微调技术的支持较慢，因为需要自行实现。</li></ul></li><li><p><strong>NeMo</strong>：由 NVIDIA 维护的微调工具，预计至少有 50 位全职开发者。</p><ul><li>优点：开发者众多，技术实力最强，该项目的 Python 代码有 43 万行，是其它几个项目的十倍以上，在一些细节实现方面做得更好，是这些工具中唯一支持 FP8 训练的。</li><li>缺点：与其它生态割裂，需要先将模型转换为特殊格式，支持的模型数量较少。作为 NVIDIA 的项目，官方不会支持其它显卡。</li></ul></li></ul><p>长期来看，笔者认为：</p><ul><li><strong>NeMo</strong> 适合对性能和稳定性要求高的大型企业。它的开发者数量最多且是全职的，后续发展有保证，但支持的模型和技术较少，生态相对封闭。</li><li><strong>TRL</strong> 适合大多数中小公司，它依赖庞大的 Transformers 社区，有许多第三方工具支持，例如 DeepSpeed，支持最多的模型和训练方法，因此将会长期存在。</li><li><strong>torchtune</strong> 介于两者之间，使用简单，其中的模型架构是自行实现的，因此能够像 NeMo 那样做许多优化，不过这也导致模型数量较少。</li></ul><p>因此，笔者建议可以先使用 TRL 及相关的库进行实验，尝试多种微调方法，等效果稳定后再迁移到 torchtune 或 NeMo 上以提升性能。</p><p>因此，本书主要使用更为流行的 LLaMA-Factory 工具，同时也会介绍 torchtune 中的相关配置。</p><h2 id="使用-llama-factory-在本地微调大模型" tabindex="-1">使用 LLaMA-Factory 在本地微调大模型 <a class="header-anchor" href="#使用-llama-factory-在本地微调大模型" aria-label="Permalink to &quot;使用 LLaMA-Factory 在本地微调大模型&quot;">​</a></h2><p>本节将介绍 LLaMA-Factory 的基本使用方法，它是目前最流行的微调工具，提供了大量微调所需的功能，包括：</p><ul><li>支持多种模型：包括语言模型、MoE 模型、多模态模型。</li><li>支持全流程训练：继续预训练、指令微调、奖励模型、DPO、ORPO 等。</li><li>多种 LoRA 优化方法，比如 LoRA+、DoRA、QLoRA 等。</li><li>集成了多种优化训练性能的方法：FlashAttention-2、Liger Kernel。</li></ul><p>运行 LLaMA-Factory 需要安装 Python 环境，具体请参考本书[Python 环境安装]，接下来是操作步骤。</p><h3 id="下载模型" tabindex="-1">下载模型 <a class="header-anchor" href="#下载模型" aria-label="Permalink to &quot;下载模型&quot;">​</a></h3><p>本书建议使用 modelscope.cn 快速下载模型，先安装 <code>pip install modelscope</code>，然后使用如下命令下载到本地：</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">modelscope</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> download</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --model</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> Qwen/Qwen2.5-0.5B-Instruct</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --local_dir</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> Qwen2.5-0.5B-Instruct</span></span></code></pre></div><p>这里我们选择 0.5B 参数量的 Qwen2.5 模型，使用它的最大好处是体积小，使得即使没有 GPU，我们也能直接使用 CPU 来运行和微调。本地操作更利于学习，后面我们将介绍如何通过断点方式学习模型架构。</p><h3 id="安装-llama-factory" tabindex="-1">安装 LLaMA-Factory <a class="header-anchor" href="#安装-llama-factory" aria-label="Permalink to &quot;安装 LLaMA-Factory&quot;">​</a></h3><p>使用如下命令安装：</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">git</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> clone</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> https://github.com/hiyouga/LLaMA-Factory.git</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">cd</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> LLaMA-Factory</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -e</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> .[torch,metrics]</span></span></code></pre></div><h3 id="准备微调数据" tabindex="-1">准备微调数据 <a class="header-anchor" href="#准备微调数据" aria-label="Permalink to &quot;准备微调数据&quot;">​</a></h3><p>创建一个新目录，比如 <code>my_train</code>，将 <code>data</code> 目录中的 <code>dataset_info.json</code> 和 <code>identity.json</code> 文件拷贝到其中。</p><p>接下来编辑 <code>identity.json</code> 文件，修改其中的 <code></code> 和 <code></code> 为你想要的名字，比如：</p><div class="language-txt vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">txt</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>You may refer to me as {{name}}, an AI assistant developed by {{author}}.</span></span></code></pre></div><p>改成：</p><div class="language-txt vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">txt</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>You may refer to me as mychat, an AI assistant developed by nwind.</span></span></code></pre></div><p>创建一个新文件，名字是 <code>train_qwen_lora.yaml</code>，内容如下所示：</p><div class="language-yaml vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">yaml</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 前面下载的模型路径</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">model_name_or_path</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">/Volumes/ai/models/Qwen2.5-0.5B-Instruct</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">stage</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">sft</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">do_train</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">true</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">finetuning_type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">lora</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">lora_target</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">all</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">dataset</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">identity</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">dataset_dir</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">my_train</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"> # 前面创建的目录</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">template</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">qwen</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">cutoff_len</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">32768</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">max_samples</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1000</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">overwrite_cache</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">true</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">preprocessing_num_workers</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">16</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">output_dir</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">qwen_train_test</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"> # 输出 LoRA 文件的目录</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">logging_steps</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">10</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">save_steps</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">500</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">plot_loss</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">true</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">overwrite_output_dir</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">true</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">per_device_train_batch_size</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">gradient_accumulation_steps</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">4</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">learning_rate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1.0e-4</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">num_train_epochs</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3.0</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">lr_scheduler_type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">cosine</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">warmup_ratio</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">bf16</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">true</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">ddp_timeout</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">180000000</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">val_size</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">per_device_eval_batch_size</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">eval_strategy</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">steps</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">eval_steps</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">500</span></span></code></pre></div><p>然后运行如下命令进行微调：</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">llamafactory-cli</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> train</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> train_qwen_lora.yaml</span></span></code></pre></div><p>即使没有 GPU，这个任务也能在 CPU 上进行训练，只需将配置文件中的 <code>bf16: true</code> 改成 <code>bf16: false</code> 即可。由于模型小且任务简单，所以即使是 CPU 也只需几分钟就能完成训练，训练后的文件将在 <code>qwen_train_test</code> 目录中。</p><p>这样你就拥有了第一个自己训练的大模型，但要怎么运行呢？</p><h3 id="运行模型" tabindex="-1">运行模型 <a class="header-anchor" href="#运行模型" aria-label="Permalink to &quot;运行模型&quot;">​</a></h3><p>创建一个新文件 <code>chat_qwen.yaml</code>，内容如下，注意修改其中的模型路径：</p><div class="language-yaml vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">yaml</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">model_name_or_path</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">/Volumes/ai/models/Qwen2.5-0.5B-Instruct</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">adapter_name_or_path</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">qwen_train_test</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"> # 需要和之前的一致</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">template</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">qwen</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#85E89D;">infer_backend</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">huggingface</span></span></code></pre></div><p>然后通过如下命令就能开启模型对话了：</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">llamafactory-cli</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> chat</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> chat_qwen.yaml</span></span></code></pre></div><p>你可以问它一句“你是谁？”虽然我们前面的训练例子是英文，但它还是能用中文回答，如下所示：</p><div class="language-txt vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">txt</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>User: 你是谁？</span></span>
<span class="line"><span>Assistant: 您好，我是 mychat，一个由 nwind 开发的人工智能助手。我旨在提供有用的回答和帮助，解答用户的问题并解决难题。</span></span></code></pre></div><p>整个过程虽然操作比前面使用平台要繁琐些，但并不复杂，只需了解简单的命令行基础知识即可。</p><p>不过你可能会有很多疑问，比如：</p><ul><li>在 YAML 文件里还有很多参数，它们的具体作用是什么？</li><li>LoRA 是什么？</li><li>训练过程中输出的 loss、learning rate 都是什么意思？</li><li>训练完后模型如何部署？</li></ul><p>本书后续章节将会解答这些疑问，让你全面了解大模型微调相关技术。</p><p>除了 LLaMA-Factory 之外，另一个笔者推荐的微调工具是 PyTorch 官方的 torchtune。它的优点是没有外部依赖，更适合研究实现原理，而且是由 PyTorch 团队维护，后续持续更新更有保障。但它目前还在开发中，功能相对 LLaMA-Factory 较少，配置格式也经常变。</p><h2 id="大模型微调可以做的事情" tabindex="-1">大模型微调可以做的事情 <a class="header-anchor" href="#大模型微调可以做的事情" aria-label="Permalink to &quot;大模型微调可以做的事情&quot;">​</a></h2><p>大模型微调是优化大模型在垂直领域能力的重要手段，得到了广泛应用，包括但不限于以下领域：</p><ul><li><strong>教育</strong>：EduChat <a href="http://arxiv.org/abs/2308.02773" target="_blank" rel="noreferrer">^danEduChatLargeScaleLanguage2023</a>、Taoli <a href="https://github.com/blcuicall/taoli" target="_blank" rel="noreferrer">^Taoli2024</a> 实现了教育领域的问答、作业批改等功能。</li><li><strong>金融</strong>：FinMA <a href="http://arxiv.org/abs/2306.05443" target="_blank" rel="noreferrer">^xiePIXIULargeLanguage2023</a>、FinGPT <a href="http://arxiv.org/abs/2306.06031" target="_blank" rel="noreferrer">^yangFinGPTOpenSourceFinancial2023</a>、Instruct-FinGPT <a href="http://arxiv.org/abs/2306.12659" target="_blank" rel="noreferrer">^zhangInstructFinGPTFinancialSentiment2023</a> 等模型实现了金融领域的问答功能。</li><li><strong>医疗</strong>：ChatDoctor <a href="http://arxiv.org/abs/2303.14070" target="_blank" rel="noreferrer">^liChatDoctorMedicalChat2023</a>、DoctorGLM <a href="http://arxiv.org/abs/2304.01097" target="_blank" rel="noreferrer">^xiongDoctorGLMFinetuningYour2023</a>、Med-PaLM <a href="https://www.nature.com/articles/s41586-023-06291-2" target="_blank" rel="noreferrer">^singhalLargeLanguageModels2023</a> 实现了医疗领域的问诊功能。</li><li><strong>法律</strong>：LawGPT <a href="http://arxiv.org/abs/2406.04614" target="_blank" rel="noreferrer">^zhouLawGPTChineseLegal2024</a>、ChatLaw <a href="http://arxiv.org/abs/2306.16092" target="_blank" rel="noreferrer">^cuiChatLawOpenSourceLegal2023</a> 及 DISC-LawLLM <a href="http://arxiv.org/abs/2309.11325" target="_blank" rel="noreferrer">^yueDISCLawLLMFinetuningLarge2023</a> 等项目实现了法律领域的咨询功能。</li><li><strong>写作</strong>：Writing-Alpaca-7B <a href="http://arxiv.org/abs/2305.13225" target="_blank" rel="noreferrer">^zhangMultiTaskInstructionTuning2023</a> 及 CoEdIT <a href="http://arxiv.org/abs/2305.09857" target="_blank" rel="noreferrer">^rahejaCoEdITTextEditing2023</a> 写作辅助工具，实现了语法纠错、简化和改写功能。</li><li><strong>软件开发</strong>：Copilot <a href="https://github.com/features/copilot" target="_blank" rel="noreferrer">^GitHubCopilotYour2024</a>、Code Llama <a href="http://arxiv.org/abs/2308.12950" target="_blank" rel="noreferrer">^roziereCodeLlamaOpen2024</a>、WizardCoder <a href="http://arxiv.org/abs/2306.08568" target="_blank" rel="noreferrer">^luoWizardCoderEmpoweringCode2023</a> 等实现了根据问题生成代码的功能。</li><li><strong>调用工具</strong>：Gorilla <a href="http://arxiv.org/abs/2305.15334" target="_blank" rel="noreferrer">^patilGorillaLargeLanguage2023</a>、GPT4Tools <a href="http://arxiv.org/abs/2305.18752" target="_blank" rel="noreferrer">^yangGPT4ToolsTeachingLarge2023</a> 及 ToolLLM <a href="http://arxiv.org/abs/2307.16789" target="_blank" rel="noreferrer">^qinToolLLMFacilitatingLarge2023</a> 等实现了让模型调用工具来完成任务的功能。</li></ul><p>而且，ChatGPT 也是通过微调获得了问答能力，因此所有能用提示词实现的功能都可以通过微调实现。同时，它的能力上限更高，可以充分发挥模型的潜力。</p><p>不过，也有一些场景不适合进行微调。笔者认为以下三种场景不建议进行微调：</p><ul><li><strong>信息变化快的场景</strong>，例如针对文档的问答。微调训练需要时间，无法做到实时更新，这种场景更适合使用 RAG（检索增强生成）。</li><li><strong>较为通用的场景</strong>，比如聊天、工具调用和代码生成等。并不是说这些功能无法微调，而是因为这些功能是基础模型的优化重点。例如，Llama 3 为了训练代码相关任务构造了 270 万条数据，我们自己进行微调的效果不一定更好，即便成功微调，也很可能在几个月后被新版的基础模型超越。</li><li><strong>有条件部署 DeepSeek V3/R1 模型</strong>，DeepSeek V3/R1 的出现让开源模型赶上了闭源模型，它在许多场景下只需提示词就能达到微调小模型的效果，所以如果有条件部署，笔者推荐优先尝试通过提示词来实现。</li></ul><h2 id="微调成本估算" tabindex="-1">微调成本估算 <a class="header-anchor" href="#微调成本估算" aria-label="Permalink to &quot;微调成本估算&quot;">​</a></h2><p>前面的微调实践表明，微调模型的成本不高，但部署成本较高。除此之外，是否还有其他成本？在这里，我们综合分析一下微调所需的成本，主要包括以下四个方面：</p><ul><li><p><strong>准备数据的人力成本</strong>：这部分成本取决于数据的量和质量。初期准备 1000 条数据的人力成本可以控制在两周以内，但数据量较大时，所需的人力成本会显著增加。</p></li><li><p><strong>数据存储成本</strong>：存储成本主要包括以下三部分：</p><ul><li><strong>指令微调数据</strong>：这部分的数据量不大，几乎可以忽略，前期可以放在一个 Excel 文件中。</li><li><strong>继续预训练文本</strong>：如果需要进行继续预训练，可能需要存储超过 100G 的文本，具体取决于领域。</li><li><strong>模型存储成本</strong>：微调后的模型需要存储，每个 13B 参数量的模型需要 26GB 的存储空间。如果进行 100 次实验，总存储需求将达到 2.6TB。然而，如果使用 LoRA 进行训练，这部分存储需求可以忽略，因为每个模型只需十几 MB。</li></ul></li><li><p><strong>训练硬件成本</strong>：虽然训练可以使用 CPU，但非常耗时，最好使用 GPU 进行训练。使用 GPU 的最低成本方法是临时租用，例如在 autodl 等平台上，4090 显卡的租用价格每小时只需 2 元。训练 1000 条数据的时间不会超过半小时，即便训练 20 次，成本也不到 40 元。因此，训练时的硬件成本不高，主要是这段时间内的人力成本。</p></li><li><p><strong>推理硬件成本</strong>：推理硬件需要持续运行，因此这部分的成本较高，具体成本与部署方式有关，主要有以下三种方案：</p><ul><li><strong>租用云 GPU 服务器</strong>：最低配置的 A10 显卡每个月需要约 4000 元，可以运行 7B 参数的模型或量化后的 14B 模型。</li><li><strong>ServerLess GPU</strong>：少量云厂商提供函数计算 GPU 服务，按量使用。如果使用频率低，成本会比租用服务器低很多，但缺点是大模型的启动时间较长，首次调用延迟高，只适合离线类的任务。</li><li><strong>购买消费级显卡 RTX 4090</strong>：整机配置在 2 万到 3 万元之间，但这类显卡不支持在数据中心使用，只适合办公网内使用。</li></ul></li></ul><p>综合来看，如果是请求量不大的垂直领域应用，微调的主要成本是人力和推理硬件。</p><h2 id="微调与-rag-的对比" tabindex="-1">微调与 RAG 的对比 <a class="header-anchor" href="#微调与-rag-的对比" aria-label="Permalink to &quot;微调与 RAG 的对比&quot;">​</a></h2><p>现阶段大模型应用最常见的方案是 RAG。RAG 的原理是先检索出与问题最相关的知识，然后将这些知识放入提示词中，让大模型根据知识进行回答。RAG 之所以有效，有一部分原因是它似乎可以看作是一种隐式微调 <a href="http://arxiv.org/abs/2212.10559" target="_blank" rel="noreferrer">^daiWhyCanGPT2023</a>，它们的注意力层输出结果与微调后的模型有一定相似性。那么，实际应用中究竟应该使用 RAG 还是微调？我们可以对比一下这两种方案的优缺点。</p><h3 id="rag-的优势" tabindex="-1">RAG 的优势 <a class="header-anchor" href="#rag-的优势" aria-label="Permalink to &quot;RAG 的优势&quot;">​</a></h3><ul><li>开发成本低，因为无需训练，可以快速支持不同场景，适合项目初期验证。</li><li>效果不差，如果底层模型能力强，通过 Few-Shot 方式，效果并不比微调差太多，甚至在某些任务下的效果还更好。</li><li>使用成本低，可以使用模型厂商提供的 API，无需部署和运维，使用成本很低。最新的大模型 API 百万 token 的价格都降到 1 元以下，甚至有的模型还提供免费版本，对于没有大量文本的场景，这个成本几乎可以忽略。</li><li>数据实时更新，只要检索结果实时更新，就能保证回答的实时性。</li></ul><h3 id="rag-的缺点" tabindex="-1">RAG 的缺点 <a class="header-anchor" href="#rag-的缺点" aria-label="Permalink to &quot;RAG 的缺点&quot;">​</a></h3><ul><li>检索召回有一定失败率，如果找不到相关文本，大模型肯定无法正确回答。</li><li>对底层模型要求高，开源模型做得好的极少。</li><li>受限于模型训练时的价值观，模型训练的对齐取决于训练的人员，导致每个模型必然有偏见，比如： <ul><li>偏向于冗长的回答，模型训练人员可能更喜欢长篇输出，因此很多大模型会生成大量冗余信息。</li><li>偏向保守的回答，模型的回答可能倾向于四平八稳，文字缺乏张力。</li><li>拒绝某些问题，模型训练为了避免风险会拒绝政治等相关敏感问题，但在某些场景下是需要的，比如小说写作。</li></ul></li><li>对于调用大模型厂商 API 的方式，会存在安全和稳定性等问题，包括： <ul><li>隐私问题，需要将数据发往第三方，无法保证数据安全，这对隐私要求高的企业（如银行、医疗等）不太适合。</li><li>稳定性难以保证，例如 OpenAI 的接口经常出故障，导致许多基于它的应用无法使用。</li><li>可能会由于厂商升级导致能力下降。由于成本考虑，模型厂商不可能保留所有模型的历史权重，新版模型可能在特定任务上不如旧版。例如，有人使用 GPT-3.5-turbo 升级版本后效果下降了 10% <a href="https://www.voiceflow.com/blog/how-much-do-chatgpt-versions-affect-real-world-performance" target="_blank" rel="noreferrer">^HowMuchChatGPT</a>。</li></ul></li><li>当上下文较长时，首 token 的速度较慢，且会占用较多的 KV 缓存。</li></ul><h3 id="微调的优势" tabindex="-1">微调的优势 <a class="header-anchor" href="#微调的优势" aria-label="Permalink to &quot;微调的优势&quot;">​</a></h3><ul><li><strong>上下文 token 数量少</strong>，不需要写较长的提示词，这也使得首个 token 延迟更低，具体性能取决于部署所使用的硬件。</li><li><strong>可以使用更小的模型，更适合私有化部署</strong>。垂直领域微调的小模型可以达到甚至超过大模型的效果，这在许多领域都得到了验证，比如下面这些例子： <ul><li>在法律领域，DISC-LawLLM <a href="http://arxiv.org/abs/2309.11325" target="_blank" rel="noreferrer">^yueDISCLawLLMFinetuningLarge2023</a> 使用 13B 模型在法律领域击败了 GPT-3.5-turbo。</li><li>在医疗领域，BioGPT <a href="http://arxiv.org/abs/2305.07804" target="_blank" rel="noreferrer">^guoImprovingSmallLanguage2023</a> 使用 1.6B 模型微调，在医疗领域的 PubMedQA 任务上准确度为 0.754，超过 GPT-4 的 0.744。</li><li>在金融领域，FinMA <a href="http://arxiv.org/abs/2306.05443" target="_blank" rel="noreferrer">^xiePIXIULargeLanguage2023</a> 通过微调 30B 模型在金融领域问答任务上超过了 GPT-4，也有人通过微调 14B 模型达到接近 GPT-4 的准确率 <a href="http://arxiv.org/abs/2408.12337" target="_blank" rel="noreferrer">^phogatFinetuningSmallerLanguage2024</a>。</li><li>在化学领域，ChemLLM <a href="https://arxiv.org/abs/2402.06852" target="_blank" rel="noreferrer">^zhangChemLLMChemicalLarge</a> 微调 7B 模型，接近 GPT-4 的准确率。</li><li>在研发领域，Anyscale <a href="https://www.anyscale.com/blog/fine-tuning-llama-2-a-comprehensive-case-study-for-tailoring-models-to-unique-applications" target="_blank" rel="noreferrer">^hakhamaneshikouroshFineTuningLlama2Tailoring</a> 通过微调 7B 模型在 SQL 生成任务上超过了 GPT-4。</li><li>在芯片领域，ChipGPT-FT <a href="http://arxiv.org/abs/2403.11202" target="_blank" rel="noreferrer">^changDataAllYou2024</a> 使用 13B 模型微调，在芯片设计领域击败了 GPT-3.5，它们的准确率分别为 70.6% 和 58.8%。</li><li>有人还在 31 种不同任务（生成标题、生成摘要、文本分类、实体识别等）上微调了多个 7B 模型 <a href="http://arxiv.org/abs/2405.00732" target="_blank" rel="noreferrer">^zhaoLoRALand3102024</a>，大部分效果都接近或超过 GPT-4。</li></ul></li><li><strong>不用等大模型更新</strong>，比如 GPT-5 可能推迟到 2025 年底甚至 2026 年。</li><li><strong>更适合结构化输出</strong>，微调模型更擅长输出复杂的 JSON 结构。虽然现在有部分大模型支持 function call 功能也能部分做到，但在层次结构较深时容易出错，且所需的 JSON Schema 很复杂，会使用大量 token。</li><li><strong>风险更可控</strong>，如果只是基于基础模型微调垂直领域功能，这个模型就只能做这一件事情，不会带有政治偏向。例如，我们微调过一个生成页面的模型，如果你问它敏感的问题，它会直接将问题转成输入框的 label，而不会回答这个问题，而有的国外开源模型可能会答错。</li><li><strong>多模态场景</strong>，以图片为例，垂直场景下的图片都很定制，通用的多模态模型难以满足需求，这时要提升效果就只能微调。例如，在博物馆问答场景下，默认 LLaVA 的准确率只有 4%，但经过微调后能达到 64% <a href="http://arxiv.org/abs/2412.01370" target="_blank" rel="noreferrer">^balaucaUnderstandingWorldsMuseums2024</a>。</li></ul><h3 id="微调的缺点" tabindex="-1">微调的缺点 <a class="header-anchor" href="#微调的缺点" aria-label="Permalink to &quot;微调的缺点&quot;">​</a></h3><ul><li><strong>开发成本高</strong>，需要构造训练数据，这里有大量人工成本。</li><li><strong>更新相对较慢</strong>，训练和部署需要较长时间，不适合对数据有实时性要求的场景。</li><li><strong>需要有一定经验</strong>，需要对大模型和微调有基本了解，本书将会介绍这些关键知识。</li><li><strong>需要训练硬件</strong>，训练通常需要使用 GPU。</li><li><strong>需要部署和运维</strong>，微调后的大模型需要部署和运维，不过部署所需算力远小于训练，对于 13B 及以下的模型可以使用 4090 进行推理，具体推理硬件选型请参考[如何选择推理显卡或加速卡]。</li></ul><h3 id="同时使用两种方法" tabindex="-1">同时使用两种方法 <a class="header-anchor" href="#同时使用两种方法" aria-label="Permalink to &quot;同时使用两种方法&quot;">​</a></h3><p>需要注意的是，RAG 和微调并不是二选一的关系，可以同时使用。例如，Gorilla <a href="http://arxiv.org/abs/2305.15334" target="_blank" rel="noreferrer">^patilGorillaLargeLanguage2023</a> 在微调之后使用 RAG 能进一步提升 12% 的效果。 另外，从商业售卖角度来看，RAG 因为门槛相对较低，许多客户自己就能做，加上现在有大量开源的 RAG 平台，同质化竞争比较严重。相对而言，微调有一定门槛，可以作为差异化功能。</p><h2 id="微调的流程" tabindex="-1">微调的流程 <a class="header-anchor" href="#微调的流程" aria-label="Permalink to &quot;微调的流程&quot;">​</a></h2><p>微调的主要流程如下图所示：</p><p><img src="/llm-finetune/assets/process.BGVBadPy.png" alt="微调流程"></p><p>主要有以下几个步骤：</p><ul><li><strong>选择基础模型</strong>，寻找适合自己任务类型的基础模型，包括模型能力及是否具备相应的知识等，具体内容将在[选择基础模型]中介绍。</li><li><strong>使用提示词</strong>，在微调前应先尝试使用提示词来完成任务，如果效果可以接受，则不需要微调。</li><li><strong>准备训练数据</strong>，准备训练数据和评估数据，并通过多次抽样确认训练数据和评估数据的质量，具体内容将在[训练数据构造及管理]中介绍。</li><li><strong>准备硬件资源</strong>，根据模型预估所需的 GPU 资源，具体内容将在[微调所需资源]中介绍。</li><li><strong>「可选」继续预训练</strong>，判断是否需要进行继续预训练，具体内容将在[继续预训练]中介绍。</li><li><strong>指令微调训练</strong>，具体内容将在[微调训练]中介绍。</li><li><strong>「可选」偏好训练</strong>，可以使用 RLHF 或 DPO，具体内容将在[偏好训练]中介绍。</li><li><strong>评估模型</strong>，对训练后的模型效果进行评估，具体内容将在[评估]中介绍。</li><li><strong>部署模型</strong>，部署模型并优化性能，具体内容将在[部署]中介绍。</li><li><strong>监控模型</strong>，持续监控模型表现和用户反馈，收集成功和失败的案例，为下一轮训练做好准备。</li></ul></div></div></main><footer class="VPDocFooter" data-v-e6f2a212 data-v-1bcd8184><!--[--><!--]--><div class="edit-info" data-v-1bcd8184><div class="edit-link" data-v-1bcd8184><a class="VPLink link vp-external-link-icon no-icon edit-link-button" href="https://github.com/nwind/llm-finetune/edit/main/start.md" target="_blank" rel="noreferrer" data-v-1bcd8184><!--[--><span class="vpi-square-pen edit-link-icon" data-v-1bcd8184></span> Edit this page<!--]--></a></div><!----></div><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-1bcd8184><span class="visually-hidden" id="doc-footer-aria-label" data-v-1bcd8184>Pager</span><div class="pager" data-v-1bcd8184><a class="VPLink link pager-link prev" href="/llm-finetune/intro.html" data-v-1bcd8184><!--[--><span class="desc" data-v-1bcd8184>Previous page</span><span class="title" data-v-1bcd8184>前言</span><!--]--></a></div><div class="pager" data-v-1bcd8184><a class="VPLink link pager-link next" href="/llm-finetune/basic.html" data-v-1bcd8184><!--[--><span class="desc" data-v-1bcd8184>Next page</span><span class="title" data-v-1bcd8184>大模型基础</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><!----><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"appendix.md\":\"EFrykACG\",\"basic.md\":\"Ce5ZVvFZ\",\"data.md\":\"89fWCx9d\",\"deploy.md\":\"CjPcg83d\",\"eval.md\":\"5Y2Fs_9V\",\"extend.md\":\"Brktvckt\",\"index.md\":\"wuTRWTff\",\"intro.md\":\"Qo0wVBT-\",\"practice.md\":\"D_29NsGa\",\"readme.md\":\"vYUVPT6v\",\"rl.md\":\"BlQlHBfj\",\"sft.md\":\"CfmN7DXA\",\"start.md\":\"BJOPEC1V\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"zh\",\"dir\":\"ltr\",\"title\":\"大模型微调与部署指南\",\"description\":\"A VitePress site\",\"base\":\"/llm-finetune/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":{\"initialValue\":\"light\"},\"themeConfig\":{\"logo\":\"/logo.png\",\"nav\":[{\"text\":\"首页\",\"link\":\"/\"}],\"sidebar\":[{\"text\":\"\",\"items\":[{\"text\":\"前言\",\"link\":\"/intro.html\"},{\"text\":\"大模型微调入门\",\"link\":\"/start.html\"},{\"text\":\"大模型基础\",\"link\":\"/basic.html\"},{\"text\":\"微调训练\",\"link\":\"/sft.html\"},{\"text\":\"对齐训练\",\"link\":\"/rl.html\"},{\"text\":\"训练数据构造及管理\",\"link\":\"/data.html\"},{\"text\":\"评估\",\"link\":\"/eval.html\"},{\"text\":\"微调实践\",\"link\":\"/practice.html\"},{\"text\":\"模型部署\",\"link\":\"/deploy.html\"},{\"text\":\"附录\",\"link\":\"/appendix.html\"},{\"text\":\"拓展阅读\",\"link\":\"/extend.html\"}]}],\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/nwind/llm-finetune\"}],\"search\":{\"provider\":\"local\",\"options\":{\"translations\":{\"button\":{\"buttonText\":\"搜索\",\"buttonAriaLabel\":\"搜索\"},\"modal\":{\"displayDetails\":\"显示详细列表\",\"resetButtonTitle\":\"重置搜索\",\"backButtonTitle\":\"关闭搜索\",\"noResultsText\":\"没有结果\",\"footer\":{\"selectText\":\"选择\",\"selectKeyAriaLabel\":\"输入\",\"navigateText\":\"导航\",\"navigateUpKeyAriaLabel\":\"上箭头\",\"navigateDownKeyAriaLabel\":\"下箭头\",\"closeText\":\"关闭\",\"closeKeyAriaLabel\":\"esc\"}}}}},\"editLink\":{\"pattern\":\"https://github.com/nwind/llm-finetune/edit/main/:path\"}},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":false}");</script>
    
  </body>
</html>